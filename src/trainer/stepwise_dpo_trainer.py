from trl import DPOTrainer
from typing import Dict, Any
import torch

class StepwiseDPOTrainer(DPOTrainer):
    def compute_loss(
        self,
        model,
        inputs: Dict[str, Any],
        return_outputs: bool = False,
        num_items_in_batch: int = None  # âœ… This is required by Hugging Face
    ):
        """
        Override compute_loss to implement stepwise reward aggregation.
        Currently, it just calls the parent method.
        """

        # TODO: implement stepwise loss aggregation logic here

        # For now, fallback to base class
        return super().compute_loss(model, inputs, return_outputs)
