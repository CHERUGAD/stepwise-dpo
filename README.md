# ğŸ§  Stepwise DPO â€” Direct Preference Optimization with LLM-based Stepwise Rewards

This project implements a custom **StepwiseDPOTrainer** that uses **LLM-generated step-wise rewards** to fine-tune a small language model using the DPO (Direct Preference Optimization) method.

Inspired by the ["Letâ€™s Verify Step by Step" (OpenAI)](https://arxiv.org/abs/2408.15240v1) paper.

---

## ğŸš€ Project Summary

| Component                    | Implementation                                     |
|-----------------------------|----------------------------------------------------|
| Base Model                  | `google/flan-t5-small` (â‰¤ 1.3B parameters)         |
| Trainer                     | `StepwiseDPOTrainer` (subclass of `trl.DPOTrainer`) |
| Reward Model                | `LLMRewardModel` using `flan-t5-small`             |
| Dataset                     | Custom synthetic pairs (`synthetic_prm.json`)      |
| Training Framework          | Hugging Face Transformers + TRL + PyTorch          |
| Inference Outputs           | `inference_results.csv`, `inference_results.jsonl` |

---

## ğŸ§© Stepwise Reward Modeling

The **LLMRewardModel** (`src/reward_model/reward_model.py`) uses a T5 model to score each reasoning step individually. Text is split into logical steps (via `\n`) and each step is scored independently using the language model.

These scores are **aggregated** and used as the reward signal during DPO training.

---

## ğŸ§ª Trainer Customization

The `StepwiseDPOTrainer`:
- Subclasses Hugging Faceâ€™s `DPOTrainer`
- Overrides `compute_loss()` to:
  - Compute full-sequence log-likelihoods
  - Compute reward differences using step-wise scores from `LLMRewardModel`
  - Apply the final loss:  
    \[
    \mathcal{L} = -\log\sigma\left((r_c - r_r) \cdot (\log p_c - \log p_r)\right)
    \]

File: `src/trainer/stepwise_dpo_trainer.py`

---

## ğŸ‹ï¸ Training Instructions

```bash
# Convert raw dataset to DPO format
python scripts/convert_to_dpo_format.py

# Train the Stepwise DPO model
python scripts/train_stepwise_dpo.py

ğŸ”§ Training Config
Epochs: 3

Batch Size: 2

Learning Rate: 5e-5

Device: CPU

Output Directory: ./outputs/final_stepwise_dpo_model

ğŸ“ˆ Inference & Evaluation
# Run batch inference with reward scoring
python scripts/inference_stepwise_dpo.py

ğŸ” Output Files
outputs/inference_results.jsonl

outputs/inference_results.csv

Each row contains:

json
Copy
Edit
{
  "prompt": "...",
  "model_output": "...",
  "chosen_score": float,
  "rejected_score": float,
  "preferred": "chosen" | "rejected"
}

ğŸ“‚ Project Structure (Trimmed)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/synthetic_prm.json
â”‚   â””â”€â”€ dpo/...
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ final_stepwise_dpo_model/
â”‚   â”œâ”€â”€ inference_results.csv
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_stepwise_dpo.py
â”‚   â”œâ”€â”€ inference_stepwise_dpo.py
â”‚   â””â”€â”€ convert_to_dpo_format.py
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ reward_model/reward_model.py
â”‚   â””â”€â”€ trainer/stepwise_dpo_trainer.py
â”œâ”€â”€ LLM_USAGE.md
â”œâ”€â”€ README.md


ğŸ§  LLM Usage & Attribution
Developed with the assistance of ChatGPT (GPT-4). Logged in LLM_USAGE.md.

